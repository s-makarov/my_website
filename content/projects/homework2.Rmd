---
categories:
- ""
- ""
date: "2017-10-31T21:28:43-05:00"
description: ""
draft: false
image: pic02.jpg
keywords: ""
slug: project_2
title: R strikes back
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, include=FALSE}

library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(infer)
library(DescTools)
library(wbstats)

```



# Climate change and temperature anomalies 


If we wanted to study climate change, we can find data on the *Combined Land-Surface Air and Sea-Surface Water Temperature Anomalies* in the Northern Hemisphere at [NASA's Goddard Institute for Space Studies](https://data.giss.nasa.gov/gistemp). The [tabular data of temperature anomalies can be found here](https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.txt)

To define temperature anomalies you need to have a reference, or base, period which NASA clearly states that it is the period between 1951-1980.

```{r weather_data, cache=TRUE}

# read the data into the file, skipping the first row
weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```

We filter only monthly data and the year from the intital dataset, as we will need only this data for further analysis. Then we transform the dataframe in a "longer" version to follow the general "tidy data" guidelines:

```{r tidyweather}

tidyweather <- weather %>% 
  select(1:13) %>% # year + 12 months form first 13 columns
  pivot_longer(cols = 2:13,
               values_to = "delta",
               names_to = "Month",
               values_drop_na = TRUE)

skim(tidyweather)

```

`tidyweather` has three variables now, one each for:

- year 
- month
- delta (temperature deviation)


## Plotting Information

Let us plot the data using a time-series scatter plot, and add a trendline. To do that, we first need to create a new variable called `date` in order to ensure that the `delta` values are plot chronologically: 

```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), Month, "1")),
         month = month(date, label=TRUE),
         year = year(date))

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    x = "",
    y = "Temperature Deviation",
    title = "Weather Anomalies"
  )

```

To analyze whether, the effect of increasing temperature is more pronounced in some months, we use `facet_wrap()` to produce a separate scatter plot for each month. Smoothing line is still used to show general trend: 

```{r facet_wrap}

ggplot(transform(tidyweather, Month = factor(Month, levels = c("Jan", "Feb", "Mar",
                                                               "Apr", "May", "Jun",
                                                               "Jul", "Aug", "Sep",
                                                               "Oct", "Nov", "Dec"))), aes(x = date, y = delta)) +
  facet_wrap(~Month) +
  geom_point() +
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    x = "",
    y = "Temperature Deviation", 
    title = "Weather Anomalies"
  )
  

```

It is sometimes useful to group data into different time periods to study historical data. For example, we often refer to decades such as 1970s, 1980s, 1990s etc. to refer to a period of time. NASA calculates a temperature anomaly, as difference form the base period of 1951-1980. The code below creates a new data frame called `comparison` that groups data in five time periods: 1881-1920, 1921-1950, 1951-1980, 1981-2010 and 2011-present.

We remove data before "1800 and before" using `filter`. Then, we use the `mutate` function to create a new variable `interval` which contains information on which period each observation belongs to. We can assign the different periods using `case_when()`:

```{r intervals}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```

Now that we have the `interval` variable, we can create a density plot to study the distribution of monthly deviations (`delta`), grouped by the different time periods we are interested in. Set `fill` to `interval` to group and colour the data by different time periods:

```{r density_plot}

ggplot(comparison, aes(x=delta, fill = interval))+
  geom_density(alpha=0.2) +   #density plot with tranparency set to 20%
  theme_bw() +                #theme
  labs (
    x = "",
    title = "Density Plot for Monthly Temperature Anomalies",
    y     = "Density"         #changing y-axis label to sentence case
  )

```

So far, we have been working with monthly anomalies. However, we might be interested in average annual anomalies. We can do this by using `group_by()` and `summarise()`, followed by a scatter plot to display the result: 

```{r averaging}

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  # creating summaries for mean delta 
  # use `na.rm=TRUE` to eliminate NA (not available) values 
  summarize(annual_average_delta = mean(delta, na.rm=TRUE)) 

#plotting the data:
ggplot(average_annual_anomaly, aes(x=Year, y= annual_average_delta))+
  geom_point()+
  #Fit the best fit line, using LOESS method
  geom_smooth() +
  #change to theme_bw() to have white background + black frame around plot
  theme_bw() +
  labs (
    title = "Average Yearly Anomaly",
    y     = "Average Annual Delta"
  )                         


```

## Confidence Interval for `delta`

[NASA points out on their website](https://earthobservatory.nasa.gov/world-of-change/decadaltemp.php) that 

> "A one-degree global change is significant because it takes a vast amount of heat to warm all the oceans, atmosphere, and land by that much. In the past, a one- to two-degree drop was all it took to plunge the Earth into the Little Ice Age."

Now we are only interested in what is happening  between 2011-present. We want to construct confidence intervals for the average annual delta in temperature for this period. We will implement two methods: usual confidence interval by formula and a bootstrap method. Let's do usual first:

```{r, calculate_CI_using_formula}

formula_ci <- comparison %>% 
  filter(interval == "2011-present") %>%  # choose the interval 2011-present using `filter` verb
  summarize(mean_t = mean(delta), sd_t = sd(delta), c_t = length(delta), se_t = sd_t/sqrt(c_t),
            low_t = mean_t - 1.96*se_t, high_t = mean_t + 1.96*se_t)
  # calculate summary statistics for temperature deviation (delta):
  # calculate mean, SD, count, SE, lower/upper 95% CI using `summarize` verb

#print out formula_CI
formula_ci

paste0("95% CI (using usual formula) for temperature anomalie since 2011 is between ", round(formula_ci[[5]],2), " and ",
       round(formula_ci[[6]],2), " degrees")

```


```{r, calculate_CI_using_bootstrap}
set.seed(1234)

boot_weather <- comparison %>%
  filter(interval == "2011-present") %>%
  specify(response = delta) %>%
  # repeat 10000 times the sampling procedure; we can do more, if we want, but it will not bring
  # much more precision here
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

CI_boot_weather <- boot_weather %>%
  get_confidence_interval(level = 0.95, type = "percentile")

paste0("95% CI (using bootstrap) for temperature anomalie since 2011 is between ", round(CI_boot_weather[[1]],2), " and ",
       round(CI_boot_weather[[2]],2), " degrees")


```
*Comment: CIs for usual formula and bootstrap are nearly identical*

> What is the data showing us?

We first plotted a scatter plot with a trend line in order to have a clear visualisation of how the temperature levels deviated from the ‘normal’ levels over time. From the graph it is clear that generally temperature deviations have been increasing over time from 1840s up until now, with a steeper consistent increase from 1970s to today. We then faceted the graph by months in order to see temperature deviations by month. This has shown as that temperature deviations have been consistent throughout the year with all month showing similar trends. We then grouped the years by periods in order to see the temperature anomalies by decade. This has shown that from 2011 onwards, annual temperatures were most likely to be 1 degree higher than normal. Moreover, each decade the expectation of the temperature deviation increases by 0.5 degrees approximately. The confidence intervals that we have constructed for the period from 2011 up until now further support this conclusion with the interval between 0.92 and 1.02, which shows statistical evidence of mean temperature being in 95% cases more than 0.92 degrees greater than in the baseline period. (In general, it follows global warming intuition)


# General Social Survey (GSS)


The [General Social Survey (GSS)](http://www.gss.norc.org/) gathers data on American society in order to monitor and explain trends in attitudes, behaviours, and attributes. Many trends have been tracked for decades, so one can see the evolution of attitudes, etc in American Society.

Here we analyze data from the **2016 GSS sample data**, using it to estimate values of *population parameters* of interest about US adults. The GSS sample data file has 2867 observations of 935 variables, but we are only interested in very few of these variables and we are using a smaller file:

```{r, read_gss_data, cache=TRUE}

gss <- read_csv(here::here("data", "smallgss2016.csv"), 
                na = c("", "Don't know",
                       "No answer", "Not applicable"))

```

Many responses should not be taken into consideration, like "No Answer", "Don't Know", "Not applicable", "Refused to Answer".

We will be creating 95% confidence intervals for population parameters. The variables we have are the following:

- hours and minutes spent on email weekly. The responses to these questions are recorded in the `emailhr` and `emailmin` variables. For example, if the response is 2.50 hours, this would be recorded as emailhr = 2 and emailmin = 30.
- `snapchat`, `instagrm`, `twitter`: whether respondents used these social media in 2016
- `sex`: Female - Male
- `degree`: highest education level attained


## Instagram and Snapchat usage by sex

Can we estimate the *population* proportion of Snapchat or Instagram users in 2016?

- Create a  new variable, `snap_insta` that is *Yes* if the respondent reported using any of Snapchat (`snapchat`) or Instagram (`instagrm`), and *No* if not. If the recorded value was NA for both of these questions, the value in your new variable should also be NA:

```{r}

Inst_Snap <- gss %>% 
  mutate(snap_insta = case_when(
    instagrm == "Yes" | snapchat == "Yes" ~ "Yes",
    instagrm == "No" & snapchat == "No" ~ "No",
    TRUE ~ NA_character_
  ))

```


- Calculate the proportion of Yes’s for `snap_insta` among those who answered the question, i.e. excluding NAs.

```{r}
si_no_na <- Inst_Snap %>% 
  filter(!is.na(snap_insta)) %>%
  count()

si_yes <- Inst_Snap %>% 
  filter(snap_insta == "Yes") %>% 
  count()

prop_us <- si_yes/si_no_na*100

paste0(round(prop_us,2), "% of respondents use either Snapchat or Instagram")
```

- Using the CI formula for proportions, please construct 95% CIs for men and women who used either Snapchat or Instagram

```{r}

sex_ttl <- Inst_Snap %>% 
  filter(!is.na(snap_insta)) %>%
  group_by(sex) %>% 
  count()

sex_yes <- Inst_Snap %>% 
  filter(snap_insta == "Yes") %>%
  group_by(sex) %>% 
  count()

male_ci <- BinomCI(sex_yes[[2,2]],sex_ttl[[2,2]], conf.level = 0.95)
female_ci <- BinomCI(sex_yes[[1,2]],sex_ttl[[1,2]], conf.level = 0.95)

paste0("95% CI for usage of Snapchat or Instagram among male respondents is between ", round(male_ci[2],2), " and ",
       round(male_ci[3],2))

paste0("95% CI for usage of Snapchat or Instagram among female respondents is between ", round(female_ci[2],2), " and ",
       round(female_ci[3],2))
```


## Twitter usage by education level

Can we estimate the *population* proportion of Twitter users by education level in 2016?

Yes, statistically it is viable to extrapolate a population (split) of users from a sufficiently large sample set. There are requirements that have to be met for this to work, such as non-overlapping confidence intervals, but generally it is possible to estimate the population. In the same way that election polls do not ask the whole country but just a very small group of people, these techniques can be applied to other measures.
Below you can find a code for mean point estimation on the given sample:

```{r}
gss <- gss %>% 
  mutate(degree = factor(degree, levels = c("Lt high school", "High school", "Junior college", "Bachelor", "Graduate"), ordered = TRUE))

gss %>% 
  filter(degree != "NA") %>% 
  filter(twitter != "NA") %>% 
  group_by(degree) %>% 
  summarize(prop_deg = sum(twitter == "Yes")/n())
```



There are 5 education levels in variable `degree` which, in ascending order of years of education, are Lt high school, High school, Junior college, Bachelor, Graduate. 

- Turn `degree` from a character variable into a factor variable. Make sure the order is the correct one and that levels are not sorted alphabetically which is what R by default does. 

```{r}

gss <- gss %>% 
  mutate(degree = factor(degree, levels = c("Lt high school", "High school", "Junior college", "Bachelor", "Graduate"), ordered = TRUE))

```

- Create a  new variable, `bachelor_graduate` that is *Yes* if the respondent has either a `Bachelor` or `Graduate` degree. As before, if the recorded value for either was NA, the value in your new variable should also be NA.

```{r}

gss <- gss %>% 
  mutate(bachelor_graduate = case_when(
    degree == "Bachelor" | degree == "Graduate" ~ "Yes",
    degree == "NA" ~ NA_character_,
    TRUE ~ "No"
  ))

```

- Calculate the proportion of `bachelor_graduate` who do (Yes) and who don't (No) use twitter. 

```{r}

tw_total <- gss %>% 
  filter(twitter != "NA") %>% 
  filter(bachelor_graduate == "Yes") %>%
  count()

tw_yes <- gss %>% 
  filter(bachelor_graduate == "Yes", twitter == "Yes") %>% 
  count()

prop_yes <- (tw_yes/tw_total*100)
prop_no <- (1-tw_yes/tw_total)*100

paste0("The proportion of Bachelors and Graduates using Twitter is ", round(prop_yes, 2), "%")
paste0("The proportion of Bachelors and Graduates not using Twitter is ", round(prop_no, 2), "%")

```

- Using the CI formula for proportions, we construct two 95% CIs for `bachelor_graduate` vs whether they use (Yes) and don't (No) use twitter.

```{r}

yes_ci <- BinomCI(as.integer(tw_yes), as.integer(tw_total), conf.level = 0.95)

no_ci <- BinomCI(as.integer(tw_total - tw_yes), as.integer(tw_total), conf.level = 0.95)

paste0("95% CI for usage of Twitter among Bachelors and Graduates is between ", round(yes_ci[2],2), " and ",
       round(yes_ci[3],2))

paste0("95% CI for Bachelors and Graduates proportion not using Twitter is between ", round(no_ci[2],2), " and ",
       round(no_ci[3],2))

```


- Do these two Confidence Intervals overlap?

No, as estimated values are significantly different (if the proportion of users was around 0.5 there would be a higher chance of overlap) and the sample size is large (making the CIs quite narrow), there is no overlap.


## Email usage

Can we estimate the *population* parameter on time spent on email weekly?

**Similarly to the question about population mean of Twitter users:**
Yes, statistically it is viable to extrapolate a population of users from a sufficiently large sample set. There are requirements that have to be met for this to work (such as non-overlapping confidence intervals). Generally it is possible to estimate the population. In the same way that election polls do not ask the whole country but just a very small group of people, these techniques can be applied to other measures.
Below you can find a code for mean point estimation on the given sample (it will be done later in more details):

```{r}
# create column with minutes spent on e-mail
m_em <- gss %>% 
  filter(emailmin != "NA", emailhr != "NA") %>% 
  mutate(email = as.numeric(emailhr)*60+as.numeric(emailmin)) %>% 
  summarise(mean(email))

paste0("Sample mean estimate of time spent on email weekly is ", round(m_em[[1]],1), " minutes")

```


- Create a new variable called `email` that combines `emailhr` and `emailmin` to reports the number of minutes the respondents spend on email weekly:

```{r}

gss_email <- gss %>% 
  filter(emailmin != "NA", emailhr != "NA") %>% 
  mutate(email = as.numeric(emailhr)*60+as.numeric(emailmin))

# gss_email$email - is the column with minutes spent on email

```

- Now we visualise the distribution of this new variable, find the mean and the median number of minutes respondents spend on email weekly. Is the mean or the median a better measure of the typical amoung of time Americans spend on email weekly? Why?

```{r}

ggplot(gss_email, aes(x = email)) +
  geom_histogram() +
  labs(x = "Minutes spent on email (per week)",
       y = "")

m_m <- gss_email %>% 
  summarize(Mean = mean(email), Median = median(email))

m_m_hum <- c(paste0(m_m[[1]]%/%60, " hr and ", round(m_m[[1]]%%60,1), " minutes"),
                 paste0(m_m[[2]]%/%60, " hr and ", round(m_m[[2]]%%60,1), " minutes"))

paste0("Mean minutes spend on email weekly is ", m_m_hum[[1]], " while the median is ", m_m_hum[[2]])
```
Median is a better measure for the minutes spent on the email here, as the data is skewed to the right and median is a more robust measure. We can observe several outliers in the data (two observations are above 5000 minutes for email per week and a few between 4000 and 5000)

- Using the `infer` package, we calculate a 95% bootstrap confidence interval for the mean amount of time Americans spend on email weekly. Interpret this interval in context of the data, reporting its endpoints in “humanized” units (e.g. instead of 108 minutes, report 1 hr and 8 minutes). If you get a result that seems a bit odd, discuss why you think this might be the case.

```{r}

set.seed(1234)

e_boot <- gss_email %>%
  specify(response = email) %>%
  generate(reps = 10000, type = "bootstrap") %>%
  calculate(stat = "mean")

CI_boot_email <- e_boot %>%
  get_confidence_interval(level = 0.95, type = "percentile")

CI_boot_hum <- c(paste0(CI_boot_email[[1]]%/%60, " hr and ", round(CI_boot_email[[1]]%%60,1), " minutes"),
                 paste0(CI_boot_email[[2]]%/%60, " hr and ", round(CI_boot_email[[2]]%%60,1), " minutes"))

paste0("95% CI for time spent on email weekly is between ", CI_boot_hum[1],  " and ", CI_boot_hum[2])

```


- Would you expect a 99% confidence interval to be wider or narrower than the interval you calculated above? Explain your reasoning.

We expect a 99% CI to be wider than a 95% CI. This is because a 99% CI includes a broader range of the total data set, and thus logically has to include at least the same or more data as the 95% CI.


# Trump's Approval Margins

As we saw in class, fivethirtyeight.com has detailed data on [all polls that track the president's approval ](https://projects.fivethirtyeight.com/trump-approval-ratings)

```{r, cache=TRUE}
# Import approval polls data
approval_polllist <- read_csv(here::here('data', 'approval_polllist.csv'))

glimpse(approval_polllist)

approval_polllist <- approval_polllist %>% 
  mutate(modeldate = as.Date(modeldate, "%m/%d/%Y"),
         startdate = as.Date(startdate, "%m/%d/%Y"),
         enddate = as.Date(enddate, "%m/%d/%Y"),
         createddate = as.Date(createddate,"%m/%d/%Y"))

```

## Plotting the data

What I would like you to do is to calculate the average net approval rate (approve- disapprove) for each week since he got into office. I want you plot the net approval, along with its 95% confidence interval. There are various dates given for each poll, please use `enddate`, i.e., the date the poll ended.

```{r, fig.width= 11, dpi = 220}

trump_ap <- approval_polllist %>% 
  filter(subgroup == "Voters") %>% 
  mutate(net_approval = (approve - disapprove),
         week = week(enddate),
         year = year(enddate)) %>% 
  group_by(year, week) %>% 
  summarise(av_net_app = mean(net_approval),
            ttl = n(),
            se_ci = sd(net_approval)/sqrt(ttl),
            year = year) %>% 
  mutate(CI_low = (av_net_app - 1.96*se_ci),
         CI_high = (av_net_app + 1.96*se_ci)) %>% 
  unique()
            


ggplot(trump_ap, aes(x = week, y = av_net_app, col = factor(year))) +
  facet_wrap(~year) +
  geom_point() +
  geom_line() +
  geom_ribbon(aes(ymin = CI_low, ymax = CI_high, fill = factor(year)), alpha=0.1) +
  theme_bw() +
  theme(legend.position="none", aspect.ratio = 0.3) +
  coord_cartesian(xlim = c(0, 52), ylim = c(-20, 7.5)) +
  labs(title = "Estimating Net Approval (approve-disapprove) for Donald Trump",
       subtitle = "Weekly average of all polls",
       x = "Week of the year",
       y = "Average Net Approval (%)")+
  geom_abline(a = 0, slope = 0, col = "orange") +
  scale_x_continuous(breaks = c(0, 13, 26, 39, 52)) +
  scale_y_continuous(n.breaks = 12)

```

You can facet by year, and add an orange line at zero. Your plot should look like this:

```{r trump_margins, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "trump_approval_margin.png"), error = FALSE)
```

## Confidence Intervals Comparison

Compare the confidence intervals for `week 15` (6-12 April 2020) and `week 34` (17-23 August 2020). Can you explain what's going on? One paragraph would be enough.

```{r}

CI_app <- trump_ap %>% 
  filter(year == 2020) %>% 
  filter(week == 15 | week == 34) %>% 
  unique()

paste0("95% CI for week 15 is between ", round(CI_app$CI_low[1],2),
       " and ", round(CI_app$CI_high[1],2))

paste0("95% CI for week 34 is between ", round(CI_app$CI_low[2],2),
       " and ", round(CI_app$CI_high[2],2))

```


We can see that the confidence interval for week 15 is between -8.16 and -6.34 while for week 34 is between -12.65 and -7.68. This suggests that there was more certainty about voters’ preference in week 34 compared to week 15. The reason may be Donald Trump’s response to coronavirus which many voters may not have been satisfied with. This could have resulted in many voters changing their perception of Donald Trump which meant decline and greater uncertainty about Trump’s approval ratings in late summer (week 34) than earlier in the year (week 15). (Statistically speaking, the CI became wider. What is more interesting, these CIs do not overlap, meaning that the net approval in week 34 is highly likely (nearly definitely) is lower than in week 15)



# Gapminder revisited


Recall the `gapminder` data frame from the gapminder package. That data frame contains just six columns from the larger [data in Gapminder World](https://www.gapminder.org/data/). In this part, we will join a few data frames with more data than the 'gapminder' package. Specifically, we will look at data on:

- Life expectancy at birth (life_expectancy_years.csv)
- GDP per capita in constant 2010 US$ (https://data.worldbank.org/indicator/NY.GDP.PCAP.KD)
- Female fertility: The number of babies per woman (https://data.worldbank.org/indicator/SP.DYN.TFRT.IN)
- Primary school enrollment as % of children attending primary school (https://data.worldbank.org/indicator/SE.PRM.NENR)
- Mortality rate, for under 5, per 1000 live births (https://data.worldbank.org/indicator/SH.DYN.MORT)
- HIV prevalence (adults_with_hiv_percent_age_15_49.csv): The estimated number of people living with HIV per 100 population of age group 15-49.

We use the `wbstats` package to download data from the World Bank. The relevant World Bank indicators are `SP.DYN.TFRT.IN`, `SE.PRM.NENR`, `NY.GDP.PCAP.KD`, and `SH.DYN.MORT`:

```{r, get_data, cache=TRUE}

# load gapminder HIV data
hiv <- read_csv(here::here("data","adults_with_hiv_percent_age_15_49.csv"))
life_expectancy <- read_csv(here::here("data","life_expectancy_years.csv"))

# get World bank data using wbstats
indicators <- c("SP.DYN.TFRT.IN","SE.PRM.NENR", "SH.DYN.MORT", "NY.GDP.PCAP.KD")


library(wbstats)

worldbank_data <- wb_data(country="countries_only", #countries only- no aggregates like Latin America, Europe, etc.
                          indicator = indicators, 
                          start_date = 1960, 
                          end_date = 2016)

# get a dataframe of information regarding countries, indicators, sources, regions, indicator topics, lending types, income levels,  from the World Bank API 
countries <-  wbstats::wb_cachelist$countries
countries <- countries[,c("iso3c", "country", "region")]

```

You have to join the 3 dataframes (life_expectancy, worldbank_data, and HIV) into one. You may need to tidy your data first and then perform [join operations](http://r4ds.had.co.nz/relational-data.html). Think about what type makes the most sense **and explain why you chose it**.

```{r}
# As we upload only data for 1960-2016 from world bank, we do not need data on life_exp from 1800 for the analysis
life_expectancy_long <- life_expectancy %>% 
  pivot_longer(cols = -1,
               names_to = "date",
               values_to = "life_exp") %>% 
  mutate(date = as.double(date)) %>% 
  filter(date > 1959)

hiv_long <- hiv %>%
  pivot_longer(cols = -1,
               names_to = "date",
               values_to = "hiv") %>% 
  mutate(date = as.double(date))

# Get a string of all country names
countries_life <- life_expectancy_long %>% 
  select(country) %>% 
  distinct() %>% 
  pull()

countries_hiv <- hiv_long %>% 
  select(country) %>% 
  distinct() %>% 
  pull()

# Use countrycode package to convert names to iso3c 
library(countrycode)
countrycodes_life <- countries_life%>% 
  countrycode(
    origin = "country.name",
    destination = "iso3c"
  )

countrycodes_hiv <- countries_hiv %>% 
  countrycode(
    origin = "country.name",
    destination = "iso3c"
  )

# create a tibble that has countries name and iso3c codes
countrycode_match_life <- tibble(
  country = countries_life, 
  iso3c = countrycodes_life
)

countrycode_match_hiv <- tibble(
  country = countries_hiv, 
  iso3c = countrycodes_hiv
)

# Add iso3c code on dataframe, using a left join
life_expectancy_long <- left_join(life_expectancy_long, countrycode_match_life, by = "country")
# clear unnecessary column
life_expectancy_long <- life_expectancy_long[-1]
hiv_long <- left_join(hiv_long, countrycode_match_hiv, by = "country")
# clear unnecessary column
hiv_long <- hiv_long[-1]

# Join life_expectancy, worldbank_data, HIV and countries
tidy_gapminder <- full_join(worldbank_data, life_expectancy_long, by = c("iso3c", "date")) %>% 
  full_join(hiv_long, by = c("iso3c", "date")) %>% 
  left_join(countries, by = "iso3c")

tidy_gapminder_1979 <- left_join(hiv_long, life_expectancy_long, by = c("iso3c", "date")) %>% 
  left_join(worldbank_data, by = c("iso3c", "date")) %>% 
  left_join(countries, by = "iso3c")


clean_tidy_gapminder <- tidy_gapminder %>% 
      rename_at(
        vars(ends_with(".x")),
        ~str_replace(., "\\..$","")
      ) %>% 
      select_at(
        vars(-ends_with(".y"))
      )

clean_tidy_gapminder_1979 <- tidy_gapminder_1979 %>% 
      rename_at(
        vars(ends_with(".x")),
        ~str_replace(., "\\..$","")
      ) %>% 
      select_at(
        vars(-ends_with(".y"))
      )

```
- For full data:
When merging life_expectancy, worldbank_data and HIV, we use full_join to keep a full range of the original data. When matching the countries information, we use left_join because we want to keep the observations from the 3 dataframes we joined previously.

- For cropped to the HIV availability set:
We use `left_join` always to get the set fitting the HIV availability. It will be used in a couple of chunks connected with HIV

- What is the relationship between HIV prevalence and life expectancy?

We generate a scatterplot with a smoothing line to analyze relationship between HIV and life expectancy. It is interesting to see the data for each of the regions, so we facet by region:

```{r, fig.width=10, dpi =200}

hiv_life <- clean_tidy_gapminder_1979 %>% 
  filter(!is.na(hiv)) %>% 
  filter(!is.na(life_exp))

ggplot(hiv_life, aes(x = hiv, y = life_exp)) +
  geom_point(size = 1) +
  geom_smooth()+
  facet_wrap(~ region, scales = "free", ncol = 2)+
  theme_bw()+
  labs (
    title = "No obvious correlation between HIV prevalence and life expectancy",
    y = "Life expectancy",
    x = "HIV prevalence (per 100 population of age group 15-49)",
    caption = "source: gapminder, World Bank"
  )

```

It is important to notice that HIV reporting has quite a poor quality with some countries reporting the same number for several years (maybe due to rounding up) and quite a few NAs is present in the data.
We can also take only one year for the graphs (for exmaple, the most recent one, which is 2011). This way we will show clearly the cross-section for this dependance:

```{r, fig.width=10, dpi =200}

hiv_life_11 <- clean_tidy_gapminder_1979 %>% 
  filter(!is.na(hiv)) %>% 
  filter(!is.na(life_exp)) %>% 
  filter(date == 2011)

ggplot(hiv_life_11, aes(x = hiv, y = life_exp)) +
  geom_point(size = 1) +
  geom_smooth()+
  facet_wrap(~ region, scales = "free", ncol = 2)+
  theme_bw()+
  labs (
    title = "No obvious correlation between HIV prevalence and life expectancy",
    y = "Life expectancy in 2011",
    x = "HIV prevalence (per 100 population of age group 15-49) in 2011",
    caption = "source: gapminder, World Bank"
  )

```

We can see that as in **North America**, **South Asia** and **Middle East & North Africa** we have few countries reporting (and reporting are similar to each other) there can be no meaningful best fit lines. 

- What is the relationship between fertility rate and GDP per capita?

We generate a scatterplot with a smoothing line and use facetting by region, which helps to understand regional trends:

```{r, fig.width=10, dpi =200}

fertility_gdp <- clean_tidy_gapminder %>% 
  filter(!is.na(SP.DYN.TFRT.IN)) %>% 
  filter(!is.na(NY.GDP.PCAP.KD))

ggplot(fertility_gdp, aes(y = SP.DYN.TFRT.IN, x = NY.GDP.PCAP.KD)) +
  geom_point(size = 1) +
  geom_smooth(method = "lm")+
  scale_x_log10() +
  facet_wrap(~ region, scales = "free", ncol = 2)+
  theme_bw()+
  labs (
    title = "Countries with higher GDP per capita tend to have lower fertility rate",
    x = "GDP per capita (in constant 2010 US$)",
    y = "Fertility rate",
    caption = "source: gapminder, World Bank"
  )

```

Creating a graph for all the datapoints looks a little messy and just for similar anlaysis we can create set of graphs for one of the latest years (2011, for example):

```{r, fig.width=10, dpi =200}

fertility_gdp_11 <- clean_tidy_gapminder %>% 
  filter(!is.na(SP.DYN.TFRT.IN)) %>% 
  filter(!is.na(NY.GDP.PCAP.KD)) %>% 
  filter(date == 2011)

ggplot(fertility_gdp_11, aes(y = SP.DYN.TFRT.IN, x = NY.GDP.PCAP.KD)) +
  geom_point(size = 1) +
  geom_smooth()+
  scale_x_log10() +
  facet_wrap(~ region, scales = "free", ncol = 2)+
  theme_bw()+
  labs (
    title = "Countries with higher GDP per capita tend to have lower fertility rate",
    x = "GDP per capita (in constant 2010 US$)",
    y = "Fertility rate",
    caption = "source: gapminder, World Bank"
  )

```

Again as previously there are few data points in **North America**, **South Asia** and **Middle East & North Africa** and there can be no meaningful best fit lines. 

- Which regions have the most observations with missing HIV data?

We generate a bar chart (`geom_col()`), in descending order. However, we want to analyse both number of missing values and percentage, as we expect for these two charts to have different leaders.

```{r, fig.width=10, dpi = 150}

count_hiv_na <- clean_tidy_gapminder_1979 %>% 
  filter(region != is.na(region)) %>% 
  group_by(region) %>% 
  summarise(hiv_na = sum(is.na(hiv)),
            na_prop = sum(is.na(hiv)/length(hiv)))

ggplot(count_hiv_na, aes(x = hiv_na, y = reorder(region, hiv_na))) + 
  geom_col() + 
  labs(title = "Sub-Saharan Africa has the highest number of missing data!",
       x = "Missing HIV data points (since 1979)",
       y= "") +
  theme_bw()

ggplot(count_hiv_na, aes(x = na_prop, y = reorder(region, na_prop))) +
  geom_col() +
  labs(title = "South Asia has the highest percentage of missing data!",
       x = "% of missing HIV data points (since 1979)",
       y= "") +
  theme_bw() +
  scale_x_continuous(label =scales:: percent)

```

- How has mortality rate for under 5 changed by region? In each region, find the top 5 countries that have seen the greatest improvement, as well as those 5 countries where mortality rates have had the least improvement or even deterioration.

```{r, fig.width = 11, dpi = 220}

five_mort <- clean_tidy_gapminder %>% 
  filter(!is.na(SH.DYN.MORT)) %>% 
  # knowing the dataset for the HIV, period between 1979 and 2011 was well-documented, so we use it here
  # as no specific guidance for the period was made beforehand
  filter(date == 1979 | date == 2011) %>% 
  select(region, date, country, SH.DYN.MORT) %>% 
  group_by(country) %>% 
  summarise(diff = SH.DYN.MORT[date == 2011] - SH.DYN.MORT[date == 1979],
            diff_re = (SH.DYN.MORT[date == 2011] - SH.DYN.MORT[date == 1979])/SH.DYN.MORT[date == 1979],
            region = region) %>% 
  unique()

top_5_abs <- five_mort %>% 
  arrange(desc(diff)) %>% 
  group_by(region) %>% 
  top_n(-5, diff)

bot_5_abs <- five_mort %>% 
  arrange(desc(diff)) %>% 
  group_by(region) %>% 
  top_n(5, diff)

ggplot(top_5_abs, aes(x = diff, y = reorder(country, -diff))) + 
  geom_col(fill = "blue", alpha = 0.75) +
  facet_wrap(~region, scales = "free") +
  labs(x = "Absolute change in mortality rate between 1971 and 2011 (per 1000 births)",
       y = "",
       title="Regional leaders in absolute change in under five mortality rate") +
  theme_bw() +
  scale_x_reverse()


ggplot(bot_5_abs, aes(x = diff, y = reorder(country, diff))) + 
  geom_col(fill = "blue", alpha = 0.75) +
  facet_wrap(~region, scales = "free") +
  labs(x = "Absolute change in mortality rate between 1971 and 2011 (per 1000 births)",
       y = "",
       title ="Regional bottom 5 countries in absolute change in under five mortality rate") +
  theme_bw() +
  scale_x_reverse()


top_5_re <- five_mort %>% 
  arrange(diff_re) %>% 
  group_by(region) %>% 
  top_n(-5, diff_re)

bot_5_re <- five_mort %>% 
  arrange(desc(diff_re)) %>% 
  group_by(region) %>% 
  top_n(5, diff_re)

ggplot(top_5_re, aes(x = diff_re, y = reorder(country, -diff_re))) + 
  geom_col(fill = "blue", alpha = 0.75) +
  facet_wrap(~region, scales = "free") +
  labs(x = "Relative change in mortality rate between 1971 and 2011 (per 1000 births)",
       y = "",
       title="Regional leaders in relative change in under five mortality rate") +
  theme_bw() +
  scale_x_reverse(label = scales::percent)

ggplot(bot_5_re, aes(x = diff_re, y = reorder(country, diff_re))) + 
  geom_col(fill = "blue", alpha = 0.75) +
  facet_wrap(~region, scales = "free") +
  labs(x = "Absolute change in mortality rate between 1971 and 2011 (per 1000 births)",
       y = "",
       title ="Regional bottom 5 countries in relative change in under five mortality rate") +
  theme_bw() +
  scale_x_reverse(label = scales::percent)

```

There is no need to sort for top 5 countries for **North America** and **South Asia** as there are less than 10 countries in each group and top-5 and bottom-5 countries overlap. It might be reasonable to create the graph without two regions and represent them separately, but faceting by region can be still used to show case the whole picture in one set of graphs.
Also one could change the *decrease* in fertility rate to a positive number to better represent intuitive *improvement* in the situation in the country.

- Is there a relationship between primary school enrollment and fertility rate?

```{r, fig.width= 10, dpi =200}

ed_fert <- clean_tidy_gapminder %>% 
  filter(!is.na(SE.PRM.NENR)) %>% 
  filter(!is.na(SP.DYN.TFRT.IN))

ggplot(ed_fert, aes(y = SP.DYN.TFRT.IN, x = SE.PRM.NENR)) +
  geom_point(size = 1) +
  geom_smooth()+
  facet_wrap(~ region, scales = "free", ncol = 2)+
  theme_bw()+
  labs (
    title = "Countries with higher enrollment to Primary Schools tend to have lower fertility rate",
    x = "Percentage of population attending Primary School",
    y = "Fertility rate",
    caption = "source: gapminder, World Bank"
  ) +
  scale_x_continuous(label = function(x) paste0(x, "%"))

```

As in previous parts, where we used the whole data set independent of a certain year, the graph might be a little too populated (as we are trying to show the panel data in 2D space). So, let's show it for a certain year (for 2011, for example):

```{r, fig.width= 10, dpi =200}

ed_fert_11 <- clean_tidy_gapminder %>% 
  filter(!is.na(SE.PRM.NENR)) %>% 
  filter(!is.na(SP.DYN.TFRT.IN)) %>% 
  filter(date == 2011)

ggplot(ed_fert_11, aes(y = SP.DYN.TFRT.IN, x = SE.PRM.NENR)) +
  geom_point(size = 1) +
  geom_smooth()+
  facet_wrap(~ region, scales = "free", ncol = 2)+
  theme_bw()+
  labs (
    title = "Cross-Section in 2011 does not show certain relationship between fertility and Primary School enrollment",
    x = "Percentage of population attending Primary School",
    y = "Fertility rate",
    caption = "source: gapminder, World Bank"
  ) +
  scale_x_continuous(label = function(x) paste0(x, "%"))
```



# Challenge 1: CDC COVID-19 Public Use Data


Let us revisit the [CDC Covid-19 Case Surveillance Data](https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf). There are well over 3 million entries of individual, de-identified patient data. Since this is a large file, I suggest you use `vroom` to load it and you keep `cache=TRUE` in the chunk options.


```{r, cache=TRUE}
# file contains 11 variables and 3.66m rows and is well over 380Mb. 
# It will take time to download

# URL link to CDC to download data
url <- "https://data.cdc.gov/api/views/vbim-akqf/rows.csv?accessType=DOWNLOAD"

covid_data <- vroom::vroom(url)%>% # If vroom::vroom(url) doesn't work, use read_csv(url)
  clean_names()

```

**Our file contains 4.48m rows, which means that the data might have been updated and it is the reason, why we do not get the identical figures with the sample chart**

Given the data we have, we produce two graphs that show death % rate:

- by age group, sex, and whether the patient had comorbidities or not

```{r, fig.width = 12, dpi = 180}

covid_comod <- covid_data %>% 
  filter(medcond_yn == "Yes" | medcond_yn == "No") %>% 
  filter(death_yn == "Yes" | death_yn == "No") %>% 
  filter(sex == "Male" | sex == "Female") %>%
  filter(age_group != "Unknown", age_group != "Missing", !is.na(age_group)) %>% 
  # filter(race_and_ethnicity_combined != "Unknown", !is.na(race_and_ethnicity_combined)) %>%
  group_by(sex, age_group, medcond_yn)  %>%
  summarise(prop = count(death_yn == "Yes")/n()) %>% 
  mutate(medcond_yn = case_when(
           medcond_yn == "Yes" ~ "With comorbidities",
           medcond_yn == "No" ~ "Without comorbidities"
         )) %>%
  mutate(prop = round(prop*100, 1))


ggplot(covid_comod, aes(x = prop, y = age_group)) +
  geom_col(fill = "#6f7ba2", alpha = 0.9) +
  facet_grid(medcond_yn ~ sex) +
  theme_bw() +
  labs(x = "",
       y = "",
       caption = "Source: CDC") +
  geom_text(aes(label = prop, x = prop + 3)) +
  scale_x_continuous(labels = function(prop) paste0(prop, "%"))

```


- by age group, sex, and whether the patient was admitted to Intensive Care Unit (ICU) or not.


```{r, fig.width = 12, dpi = 180}

covid_icu <- covid_data %>% 
  filter(icu_yn == "Yes" | icu_yn == "No",
         death_yn == "Yes" | death_yn == "No"
         ) %>% 
  filter(sex == "Male" | sex == "Female") %>% 
  filter(age_group != "Unknown", !is.na(age_group)) %>% 
  filter(race_and_ethnicity_combined != "Unknown", !is.na(race_and_ethnicity_combined)) %>%
  group_by(sex, icu_yn, age_group, death_yn) %>% 
  summarize(ttl = n()) %>% 
  mutate(prop = ttl/sum(ttl),
         icu_yn = case_when(
           icu_yn == "Yes" ~ "Admitted to ICU",
           icu_yn == "No" ~ "No ICU"
         )) %>% 
  filter(death_yn == "Yes") %>% 
  mutate(prop = round(prop*100, 1))

ggplot(covid_icu, aes(x = prop, y = age_group)) +
  geom_col(fill = "salmon", alpha = 0.85) +
  facet_grid(icu_yn ~ sex) +
  theme_bw() +
  labs(title = "Covid death % by age group, sex and whether patient was admitted to ICU",
       x = "",
       y = "",
       caption = "Source: CDC") +
  geom_text(aes(label = prop, x = prop + 3)) +
  scale_x_continuous(labels = function(prop) paste0(prop, "%"))

```



```{r covid_challenge, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "covid_death_rate_comorbidities.png"), error = FALSE)
knitr::include_graphics(here::here("images", "covid_death_rate_icu.png"), error = FALSE)
```



# Challenge 2: Excess rentals in TfL bike sharing


Recall the TfL data on how many bikes were hired every single day. We can get the latest data by running the following

```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day)) %>% 
  filter(year >= 2015) %>% 
# not the prettiest implementation, but achieves the initial goal
  group_by(year, month) %>% 
  mutate(avg_bike_hires_month = mean(bikes_hired)) %>% 
  ungroup() %>% 
  group_by(month) %>% 
  mutate(avg_bike_hires_month_yr = mean(bikes_hired)) %>% 
  ungroup() %>% 
  group_by(year, week) %>% 
  mutate(avg_bike_hires_wk = mean(bikes_hired)) %>% 
  ungroup() %>% 
  group_by(week) %>% 
  mutate(avg_bike_hires_wk_yr = mean(bikes_hired)) %>% 
  ungroup() %>% 
  mutate(pct_chg_hires_wk = avg_bike_hires_wk/avg_bike_hires_wk_yr-1)

bike2 <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day)) %>% 
  filter(year >= 2015) %>% 
# not the prettiest implementation, but achieves the initial goal
  group_by(year, month) %>% 
  mutate(avg_bike_hires_month = median(bikes_hired)) %>% 
  ungroup() %>% 
  group_by(month) %>% 
  mutate(avg_bike_hires_month_yr = median(bikes_hired)) %>% 
  ungroup() %>% 
  group_by(year, week) %>% 
  mutate(avg_bike_hires_wk = median(bikes_hired)) %>% 
  ungroup() %>% 
  group_by(week) %>% 
  mutate(avg_bike_hires_wk_yr = median(bikes_hired)) %>% 
  ungroup() %>% 
  mutate(pct_chg_hires_wk = avg_bike_hires_wk/avg_bike_hires_wk_yr-1)

```

We can easily create a facet grid that plots bikes hired by month and year.

```{r tfl_month_year_grid, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_distributions_monthly.png"), error = FALSE)
```

Look at May and Jun and compare 2020 with the previous years. What's happening?

Because of lockdown, there were far fewer bike rentals in the given months. Stores were closed, restaurants and other public attractions too, meaning there was a lower demand for inner-city cycle hires in general. Still we can see that there are more extreme values in 2020 than in previous years (i.e. there are some days, when more than 60k bikes were hired, which was not observed previously). This might occur due to the fact that people try to avoid public transportation and more citizens begin to use bike hires. 

However, the challenge I want you to work on is to reproduce the following two graphs.

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_monthly.png"), error = FALSE)
```

```{r}
# Note to Kostis: In order to show the mean vs. median difference for the graphs I added an identifier in brackets within the graph title. I hope this will not lead to a deduction in points from reproducing the graphs.
```


```{r, fig.width = 11, dpi = 220}
mean1 <- ggplot(bike, aes(x=month, y=avg_bike_hires_month_yr, group=year)) +
  geom_line(aes(y=avg_bike_hires_month_yr), color="blue", size = 0.8) +
  geom_ribbon(aes(ymin = ifelse(avg_bike_hires_month_yr < avg_bike_hires_month, avg_bike_hires_month_yr , avg_bike_hires_month_yr - (avg_bike_hires_month_yr - avg_bike_hires_month)), 
                  ymax = avg_bike_hires_month_yr), 
                  fill="red", 
                  alpha = 0.3, 
                  color = "black", 
                  size = 0.2) +
  geom_ribbon(aes(ymin = avg_bike_hires_month_yr, 
                  ymax = ifelse(avg_bike_hires_month_yr > avg_bike_hires_month, avg_bike_hires_month_yr, avg_bike_hires_month_yr + (avg_bike_hires_month - avg_bike_hires_month_yr))), 
                  fill="green", 
                  alpha = 0.3, 
                  color = "black", 
                  size = 0.2) +
  facet_wrap(~year) +
  labs(
                  title = "Monthly changes in TfL bike rentals", 
                  subtitle = "Change from monthly average (mean) show in blue \nand calculated between 2015-2020",
  # @Kostis: I know the graph subtitle says until 2019, but I simply  could not get myself to not be accurate. Please forgive me
                  y = "Bike rentals",
                  x = "",
                  caption = "Source: TfL, London Data Store"
  ) + 
  theme_minimal() +
  theme(
                  axis.text.x = element_text(size = 6),
                  plot.caption = element_text(size = 6),
                  plot.subtitle = element_text(size = 10)
  )

mean1
```

```{r, fig.width = 11, dpi = 220}
# median
median1 <- ggplot(bike2, aes(x=month, y=avg_bike_hires_month_yr, group=year)) +
  geom_line(aes(y=avg_bike_hires_month_yr), color="blue", size = 0.8) +
  geom_ribbon(aes(ymin = ifelse(avg_bike_hires_month_yr < avg_bike_hires_month, avg_bike_hires_month_yr , avg_bike_hires_month_yr - (avg_bike_hires_month_yr - avg_bike_hires_month)), 
                  ymax = avg_bike_hires_month_yr), 
                  fill="red", 
                  alpha = 0.3, 
                  color = "black", 
                  size = 0.2) +
  geom_ribbon(aes(ymin = avg_bike_hires_month_yr, 
                  ymax = ifelse(avg_bike_hires_month_yr > avg_bike_hires_month, avg_bike_hires_month_yr, avg_bike_hires_month_yr + (avg_bike_hires_month - avg_bike_hires_month_yr))), 
                  fill="green", 
                  alpha = 0.3, 
                  color = "black", 
                  size = 0.2) +
  facet_wrap(~year) +
  labs(
                  title = "Monthly changes in TfL bike rentals", 
                  subtitle = "Change from monthly average (median) show in blue \nand calculated between 2015-2020",
  # @Kostis: I know the graph subtitle says until 2019, but I simply  could not get myself to not be accurate. Please forgive me
                  y = "Bike rentals",
                  x = "",
                  caption = "Source: TfL, London Data Store"
  ) + 
  theme_minimal() +
  theme(
                  axis.text.x = element_text(size = 6),
                  plot.caption = element_text(size = 6),
                  plot.subtitle = element_text(size = 10)
  )

median1
```


The second one looks at percentage changes from the expected level of weekly rentals. The two grey shaded rectangles correspond to the second (weeks 14-26) and fourth (weeks 40-52) quarters.

```{r tfl_percent_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_weekly.png"), error = FALSE)
```

```{r, fig.width = 11, dpi = 220}
# mean
mean2 <- ggplot(bike, mapping = aes(x = week, group = year)) +
    geom_rect(
            xmin = 13, 
            xmax = 26, 
            ymin = -0.7, 
            ymax = 0.7, 
            fill = "#ededed", 
            color = "NA",
            alpha = 0.05) + 
  geom_rect(
            xmin = 39, 
            xmax = 53, 
            ymin = -0.7, 
            ymax = 0.7, 
            fill = "#ededed", 
            color = "NA",
            alpha = 0.05) +
  geom_line(bike, mapping = 
            aes(y = pct_chg_hires_wk), 
            size = 0.35) +
  geom_ribbon(bike, mapping = 
            aes(
              ymax = pmax(pct_chg_hires_wk, 0), 
              ymin = 0), 
            alpha = 0.3, 
            fill = "green", 
            size = 0.5) +
  geom_ribbon(bike, mapping = 
            aes(
              ymin = pmin(pct_chg_hires_wk, 0), 
              ymax = 0), 
            alpha = 0.3, 
            fill = "red", 
            size = 0.5) +
  geom_rug(color = ifelse(bike$pct_chg_hires_wk>0, "green", "red"), size = 0.2) +
  facet_wrap(~year) +
  labs(
            title = "Weekly changes in TfL bike rentals",
            subtitle = "% change from weekly averages (mean) \ncalculated between 2015-2020",
# @Kostis: I know the graph subtitle says until 2019, but I simply  could not get myself to not be accurate. Please forgive me.
            x = "week",
            y = "",
            caption = "Source: TfL, London Data Store") +
  theme_bw() +
  theme_minimal() +
  theme(
            panel.grid.major = element_line(colour = "grey90", size=.2),
            panel.grid.minor = element_line(colour = "grey90", size=.2),
            plot.caption = element_text(size = 6),
            plot.subtitle = element_text(size = 8),
            plot.title = element_text(size = 10)) +
  scale_x_continuous(breaks = c(13, 26, 39, 53)) +
  scale_y_continuous(breaks = c(-0.6, -0.3, 0, 0.3, 0.6),
                     labels = c("-60%", "-30%", "0%", "30%", "60%"),
                     limits = c(-0.6, 0.6))

mean2
```

```{r, fig.width = 11, dpi = 220}
# median
median2 <- ggplot(bike2, mapping = aes(x = week, group = year)) +
    geom_rect(
            xmin = 13, 
            xmax = 26, 
            ymin = -0.7, 
            ymax = 0.7, 
            fill = "#ededed", 
            color = "NA",
            alpha = 0.05) + 
  geom_rect(
            xmin = 39, 
            xmax = 53, 
            ymin = -0.7, 
            ymax = 0.7, 
            fill = "#ededed", 
            color = "NA",
            alpha = 0.05) +
  geom_line(bike, mapping = 
            aes(y = pct_chg_hires_wk), 
            size = 0.35) +
  geom_ribbon(bike, mapping = 
            aes(
              ymax = pmax(pct_chg_hires_wk, 0), 
              ymin = 0), 
            alpha = 0.3, 
            fill = "green", 
            size = 0.5) +
  geom_ribbon(bike, mapping = 
            aes(
              ymin = pmin(pct_chg_hires_wk, 0), 
              ymax = 0), 
            alpha = 0.3, 
            fill = "red", 
            size = 0.5) +
  geom_rug(color = ifelse(bike$pct_chg_hires_wk>0, "green", "red"), size = 0.2) +
  facet_wrap(~year) +
  labs(
            title = "Weekly changes in TfL bike rentals",
            subtitle = "% change from weekly averages (median) \ncalculated between 2015-2020",
# @Kostis: I know the graph subtitle says until 2019, but I simply  could not get myself to not be accurate. Please forgive me.
            x = "week",
            y = "",
            caption = "Source: TfL, London Data Store") +
  theme_bw() +
  theme_minimal() +
  theme(
            panel.grid.major = element_line(colour = "grey90", size=.2),
            panel.grid.minor = element_line(colour = "grey90", size=.2),
            plot.caption = element_text(size = 6),
            plot.subtitle = element_text(size = 8),
            plot.title = element_text(size = 10)) +
  scale_x_continuous(breaks = c(13, 26, 39, 53)) +
  scale_y_continuous(breaks = c(-0.6, -0.3, 0, 0.3, 0.6),
                     labels = c("-60%", "-30%", "0%", "30%", "60%"),
                     limits = c(-0.6, 0.6))

median2
```

For both of these graphs, you have to calculate the expected number of rentals per week or month between 2015-2019 and then, see how each week/month of 2020 compares to the expected rentals. Think of the calculation `excess_rentals = actual_rentals - expected_rentals`. 

Should you use the mean or the median to calculate your expected rentals? Why?

For the data given here, the mean compared to the median data produces more usable results, as the lines are less erratic, and thus show the trends. Given the data per week is taken from only seven values, the median runs a risk of not being the ideal representation of the data that was seen. That being said, looking at the below graph comparison shows that the difference betwwen using a mean vs. median in this case leads to a practically imperceptible difference in the data. The median data leads to a slightly more pronounced variation in the deviation from the average shown in the first graph.

```{r fig.width = 14, dpi = 300}
library(gridExtra)

grid.arrange(mean1, median1, mean2, median2, ncol=2, nrow=2)

```




# Details

- Who did you collaborate with: Benedikt Jaletzke, Stanislav Makarov, Mark Negodyuk, Olivia Zhang, Tom Tian, Kateryna Tarasova
- Approximately how much time did you spend on this problem set: 9-10 hours
- What, if anything, gave you the most trouble: Again graph formatting (but now a little less); new issue was adapting to calculation of proportions within the table; issues with replicating graphs in identical manner

