---
title: "Session 2: Homework 1"
author: "Group A8 (Benedikt Jaletzke; Kateryna Tarasova; Tom Tian; Mark Negodyuk; Olivia Zhang; Stanislav Makarov)"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(rvest)    # scrape websites
library(purrr)  
library(lubridate) #to handle dates
library(ggthemes)
library(ggrepel)
library(ggpubr)

```



# Where Do People Drink The Most Beer, Wine And Spirits?

Back in 2014, [fivethiryeight.com](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/) published an article on alchohol consumption in different countries. The data `drinks` is available as part of the `fivethirtyeight` package. Make sure you have installed the `fivethirtyeight` package before proceeding.


```{r, load_alcohol_data}
library(fivethirtyeight)
data(drinks)


# or download directly
# alcohol_direct <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv")

```


What are the variable types? Any missing values we should worry about? 

```{r glimpse_skim_data}

skim(drinks)

# Variable types are: 4 numeric and 1 character
# n_missing is 0, so there are no missing values to worry about

```


Make a plot that shows the top 25 beer consuming countries

```{r beer_plot}

beer <- drinks %>% 
  arrange(desc(beer_servings)) %>% 
  head(25)

ggplot(beer, aes(y = reorder(country, beer_servings), x = beer_servings)) +
  geom_col() +
  labs(y = "",
       x = "Servings of Beer",
       title = "Beer Servings across Countries",
       caption = "source: fivethirtyeight") +
  theme_minimal()
  

```

Make a plot that shows the top 25 wine consuming countries

```{r wine_plot}

wine <- drinks %>% 
  arrange(desc(wine_servings)) %>% 
  head(25)

ggplot(wine, aes(y = reorder(country, wine_servings), x = wine_servings)) +
  geom_col() +
  labs(y = "",
       x = "Servings of Wine",
       title = "Wine Servings across Countries",
       caption = "source: fivethirtyeight") +
  theme_minimal()

```

Finally, make a plot that shows the top 25 spirit consuming countries
```{r spirit_plot}

spirits <- drinks %>% 
  arrange(desc(spirit_servings)) %>% 
  head(25)

ggplot(spirits, aes(y = reorder(country, spirit_servings), x = spirit_servings)) +
  geom_col() +
  labs(y = "",
       x = "Servings of Spirits",
       title = "Spirits Servings across Countries",
       caption = "source: fivethirtyeight") +
  theme_minimal()

```

What can you infer from these plots? Don't just explain what's in the graph, but speculate or tell a short story (1-2 paragraphs max).

> TYPE YOUR ANSWER AFTER (AND OUTSIDE!) THIS BLOCKQUOTE.

The trends for three types of alcohol consumption are similar in the amount distribution while different in the list countries. All top 25 Beer countries have more than 200 servings consumed, while the average for the top 25 Beer and Wine countries are also close to 200 servings. Grenada, the highest spirits-consuming country, has a consumption greater than the highest of Beer (Namibia) and Wine (France). The high consumption of beer by Namibia may relate to its colonial history by German. 

From a geographical perspective, spirits' major consumption is in developing countries, likely due to spirits' high alcohol content per volume. Two major country groups are ex-soviet countries, which consume Vodka and Caribbean countries, which drink Rum. A detailed look at the types of spirits may reveal differences in country appetites. On the contrary, developed European countries consumed most wine. These countries are both major producers and consumers for wines. Consumption for beer spread across different continent and economic levels, indicating beer as a widely accepted alcohol in the world.


# Analysis of movies- IMDB dataset

We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset)

  
```{r,load_movies, warning=FALSE, message=FALSE}

movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)

```

Besides the obvious variables of `title`, `genre`, `director`, `year`, and `duration`, the rest of the variables are as follows:

- `gross` : The gross earnings in the US box office, not adjusted for inflation
- `budget`: The movie's budget 
- `cast_facebook_likes`: the number of facebook likes cast members received
- `votes`: the number of people who voted for (or rated) the movie in IMDB 
- `reviews`: the number of reviews for that movie
- `rating`: IMDB average rating 

## Use your data import, inspection, and cleaning skills to answer the following:

- Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries?

```{r}
skim(movies)

# n_missing is 0 for all variables, so there are no NAs

movies %>% 
  distinct(title) %>% 
  count()

# There are 54 movies with duplicating titles


```

- Produce a table with the count of movies by genre, ranked in descending order

```{r}

movies %>% 
  group_by(genre) %>% 
  count() %>% 
  arrange(desc(n))

```


- Produce a table with the average gross earning and budget (`gross` and `budget`) by genre. Calculate a variable `return_on_budget` which shows how many $ did a movie make at the box office for each $ of its budget. Ranked genres by this `return_on_budget` in descending order

```{r}
averages <- movies %>% 
  group_by(genre) %>% 
  summarise(av_gross = mean(gross), av_budget = mean(budget))

averages %>% 
  mutate(return_on_budget = av_gross/av_budget) %>% 
  arrange(desc(return_on_budget))


```


- Produce a table that shows the top 15 directors who have created the highest gross revenue in the box office. Don't just show the total gross amount, but also the mean, median, and standard deviation per director.

```{r}
movies %>% 
  group_by(director) %>% 
  summarise(total = sum(gross),
            mean = mean(gross),
            median = median(gross),
            st_d = StdDev(gross)) %>% 
  arrange(desc(total)) %>% 
  head(15)


```


- Finally, ratings. Produce a table that describes how ratings are distributed by genre. We don't want just the mean, but also, min, max, median, SD and some kind of a histogram or density graph that visually shows how ratings are distributed. 

```{r}
movies %>% 
  group_by(genre) %>% 
  summarise(mean_r = mean(rating),
            min_r = min(rating),
            median_r = median(rating),
            max_r = max(rating),
            st_d = StdDev(rating))

ggplot(movies, aes(rating)) +
  geom_density() +
  facet_wrap(~genre, scales = "free_y")

# ggplot(movies, aes(rating)) +
#   geom_histogram() +
#   facet_wrap(~genre, scales = "free_y")


```


## Use `ggplot` to answer the following

  - Examine the relationship between `gross` and `cast_facebook_likes`. Produce a scatterplot and write one sentence discussing whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. What variable are you going to map to the Y- and X- axes?
  
  
```{r, gross_on_fblikes}

# On X-axis we put Facebook Likes (as it is an explanatory variable in this case)
# On Y-axis we put Gross Earnings in box office in the US (as a dependant variable)

ggplot(movies, aes(x = cast_facebook_likes, y = gross)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_log10() +
  scale_y_log10()

cor(movies$cast_facebook_likes, movies$gross)

```

Facebook likes is quite poor predictor for the box office in the US, as correlation is weak. The facebook likes variable is clustered around a single point, while gross earnings are more scattered.

 
  - Examine the relationship between `gross` and `budget`. Produce a scatterplot and write one sentence discussing whether budget is likely to be a good predictor of how much money a movie will make at the box office.

```{r, gross_on_budget}
ggplot(movies, aes(x = budget, y = gross)) +
   geom_point() +
  geom_smooth(method = "lm") +
  scale_x_log10() +
  scale_y_log10()

cor(movies$budget, movies$gross)
```
  
  Budget is a moderate predictor to the box office in the US. For high budget movies it is a better predictor.
  
  
  - Examine the relationship between `gross` and `rating`. Produce a scatterplot, faceted by `genre` and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Is there anything strange in this dataset?

```{r, gross_on_rating}
ggplot(movies, aes(x = rating, y = gross)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_y_log10() +
  facet_wrap(~genre)

```

The dataset has an initial problem for this kind of analysis in that some genres – thrillers, musicals, westerns – have so few datapoint that any analysis bears no fruit. That being said, those datasets with a more full picture show a generally quite robust correlation between rating and movie gross. This is especially true for Action and Adventure movies, which have the most clear trend, while, for example, comedy movies show more significant drops in gross revenue.
Of course, looking at IMDb ratings comes with the caveat that ratings are typically most common in the years after a movie has been released. Movies that grossed highly during their release, while already tending to be popular, are likely to be watched more, and may therefore experience a rise in ratings. Similarly, movies may become cult classics, and through this achieve high ratings later on, even if they had an unsuccessful economic performance.


# Returns of financial stocks


> You may find useful the material on [finance data sources](https://mfa2021.netlify.app/reference/finance_data/). 

We will use the `tidyquant` package to download historical data of stock prices, calculate returns, and examine the distribution of returns. 

We must first identify which stocks we want to download data for, and for this we must know their ticker symbol; Apple is known as AAPL, Microsoft as MSFT, McDonald's as MCD, etc. The file `nyse.csv` contains 508 stocks listed on the NYSE, their ticker `symbol`, `name`, the IPO  (Initial Public Offering) year, and the sector and industry the company is in.


```{r load_nyse_data, message=FALSE, warning=FALSE}
nyse <- read_csv(here::here("data","nyse.csv"))

```

Based on this dataset, create a table and a bar plot that shows the number of companies per sector, in descending order

```{r companies_per_sector}

sector_count <- nyse %>%
  group_by(sector) %>% 
  count() %>% 
  arrange(desc(n))

ggplot(sector_count, aes(x = n, y = reorder(sector, n))) +
  geom_col() +
  labs(x = "",
       y = "Sector")

```

Next, let's choose the [Dow Jones Industrial Average (DJIA)](https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average) stocks and their ticker symbols and download some data. Besides the thirty stocks that make up the DJIA, we will also add `SPY` which is an SP500 ETF (Exchange Traded Fund).


```{r, tickers_from_wikipedia}

djia_url <- "https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average"

#get tables that exist on URL
tables <- djia_url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called djia. 
# Use purr::map() to create a list of all tables in URL
djia <- map(tables, . %>% 
               html_table(fill=TRUE)%>% 
               clean_names())


# constituents
table1 <- djia[[2]] %>% # the second table on the page contains the ticker symbols
  mutate(date_added = ymd(date_added),
         
         # if a stock is listed on NYSE, its symbol is, e.g., NYSE: MMM
         # We will get prices from yahoo finance which requires just the ticker
         
         # if symbol contains "NYSE*", the * being a wildcard
         # then we jsut drop the first 6 characters in that string
         ticker = ifelse(str_detect(symbol, "NYSE*"),
                          str_sub(symbol,7,11),
                          symbol)
         )

# we need a vector of strings with just the 30 tickers + SPY
tickers <- table1 %>% 
  select(ticker) %>% 
  pull() %>% # pull() gets them as a sting of characters
  c("SPY") # and lets us add SPY, the SP500 ETF

```




```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
# Notice the cache=TRUE argument in the chunk options. Because getting data is time consuming, # cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks <- tickers %>% 
  tq_get(get  = "stock.prices",
         from = "2000-01-01",
         to   = "2020-08-31") %>%
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame
```

Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.


```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

Create a dataframe and assign it to a new object, where you summarise monthly returns since 2017-01-01 for each of the stocks and `SPY`; min, max, median, mean, SD.

```{r summarise_monthly_returns}

st_data <- myStocks_returns_monthly %>% 
  filter(date > "2017-01-01") %>% 
  group_by(symbol) %>%
  summarise(min_r = min(monthly_returns), max_r = max(monthly_returns), median_r = median(monthly_returns),
            mean_r = mean(monthly_returns), sd_r = STDEV(monthly_returns))


```


Plot a density plot, using `geom_density()`, for each of the stocks
```{r density_monthly_returns}

ggplot(myStocks_returns_monthly, aes(monthly_returns)) +
  geom_density() +
  facet_wrap(~symbol) +
  xlim(-0.5, 0.5)


```

What can you infer from this plot? Which stock is the riskiest? The least risky? 

> TYPE YOUR ANSWER AFTER (AND OUTSIDE!) THIS BLOCKQUOTE.

The riskiest stock should have the highest variance and graphically it implies lowest peak with fat tails. Moreover, skewness of the returns should be taken into consideration. According to these criteria DOW seems to be the riskiest stock out of the chosen sample.
The least risky asset is SPY, which is expected as it is a diversified index (with low idiosyncratic risk).

Finally, produce a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. Please use `ggrepel::geom_text_repel()` to label each stock with its ticker symbol

```{r risk_return_plot}

ggplot(st_data, aes(x = sd_r, y = mean_r)) +
  geom_point() +
  geom_text_repel(label = st_data$symbol)


```

What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?

> TYPE YOUR ANSWER AFTER (AND OUTSIDE!) THIS BLOCKQUOTE.

Most stocks are clustered around a mean return of ___ and a standard deviation of ___. We can see that DOW and BA have a higher standard deviation than other stocks implying a higher level of risk. While the mean return is similar to most other stocks, which suggests a worse risk-return trade-off. On the other hand, MSFT, AAPL and CRM have a similar standard deviation to most stocks while having a much higher mean return making them a more preferable choice. There are also 2 stocks (WBA and CVX) for which the mean returns are below zero, while the risk profile as represented by standard deviation is similar to the main cluster of stocks.




# On your own: IBM HR Analytics


For this task, you will analyse a data set on Human Resoruce Analytics. The [IBM HR Analytics Employee Attrition & Performance data set](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset) is a fictional data set created by IBM data scientists.  Among other things, the data set includes employees' income, their distance from work, their position in the company, their level of education, etc. A full description can be found on the website.


First let us load the data

```{r}

hr_dataset <- read_csv(here::here("data", "datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.csv"))
glimpse(hr_dataset)

```

I am going to clean the data set, as variable names are in capital letters, some variables are not really necessary, and some variables, e.g., `education` are given as a number rather than a more useful description


```{r}

hr_cleaned <- hr_dataset %>% 
  clean_names() %>% 
  mutate(
    education = case_when(
      education == 1 ~ "Below College",
      education == 2 ~ "College",
      education == 3 ~ "Bachelor",
      education == 4 ~ "Master",
      education == 5 ~ "Doctor"
    ),
    environment_satisfaction = case_when(
      environment_satisfaction == 1 ~ "Low",
      environment_satisfaction == 2 ~ "Medium",
      environment_satisfaction == 3 ~ "High",
      environment_satisfaction == 4 ~ "Very High"
    ),
    job_satisfaction = case_when(
      job_satisfaction == 1 ~ "Low",
      job_satisfaction == 2 ~ "Medium",
      job_satisfaction == 3 ~ "High",
      job_satisfaction == 4 ~ "Very High"
    ),
    performance_rating = case_when(
      performance_rating == 1 ~ "Low",
      performance_rating == 2 ~ "Good",
      performance_rating == 3 ~ "Excellent",
      performance_rating == 4 ~ "Outstanding"
    ),
    work_life_balance = case_when(
      work_life_balance == 1 ~ "Bad",
      work_life_balance == 2 ~ "Good",
      work_life_balance == 3 ~ "Better",
      work_life_balance == 4 ~ "Best"
    )
  ) %>% 
  select(age, attrition, daily_rate, department,
         distance_from_home, education,
         gender, job_role,environment_satisfaction,
         job_satisfaction, marital_status,
         monthly_income, num_companies_worked, percent_salary_hike,
         performance_rating, total_working_years,
         work_life_balance, years_at_company,
         years_since_last_promotion)


glimpse(hr_cleaned)

```

Produce a one-page summary describing this dataset. Here is a non-exhaustive list of questions:

1. How often do people leave the company (`attrition`) - 0.161 or 16.1%

```{r}
skim(hr_cleaned)


leave_comp <- hr_cleaned %>% 
  filter(attrition == "Yes") %>% 
  count()
# leave_comp <- count(hr_cleaned[hr_cleaned$attrition == "Yes",])
leave_share <- leave_comp/count(hr_cleaned)*100
leave_share
```


1. How are `age`, `years_at_company`, `monthly_income` and `years_since_last_promotion` distributed? can you roughly guess which of these variables is closer to Normal just by looking at summary statistics?

```{r}

age_stat <- summary(hr_cleaned$age)
age_stat <- c(age_stat, Sd = STDEV(hr_cleaned$age))
age_stat

yac_stat <- summary(hr_cleaned$years_at_company)
yac_stat <- c(yac_stat, Sd = STDEV(hr_cleaned$years_at_company))
yac_stat

mi_stat <- summary(hr_cleaned$monthly_income)
mi_stat <- c(mi_stat, Sd = STDEV(hr_cleaned$monthly_income))
mi_stat

yslp_stat <- summary(hr_cleaned$years_since_last_promotion)
yslp_stat <- c(yslp_stat, Sd = STDEV(hr_cleaned$years_since_last_promotion))
yslp_stat


ggplot(hr_cleaned, aes(age)) +
  geom_density()

ggplot(hr_cleaned, aes(years_at_company)) +
  geom_density()

ggplot(hr_cleaned, aes(monthly_income)) +
  geom_density()

ggplot(hr_cleaned, aes(years_since_last_promotion)) +
  geom_density()


```


1. How are `job_satisfaction` and `work_life_balance` distributed? Don't just report counts, but express categories as % of total

```{r}
workrs_num <- length(hr_cleaned$job_satisfaction)

satisf <- hr_cleaned %>% 
  group_by(job_satisfaction) %>% 
  count() %>% 
  arrange(n) %>% 
  mutate(share = n/workrs_num*100)

satisf$job_satisfaction <- factor(satisf$job_satisfaction, levels = c("Low", "Medium", "High", "Very High"))
satisf <- satisf[order(satisf$job_satisfaction), ]
satisf


wl_balance <- hr_cleaned %>% 
  group_by(work_life_balance) %>% 
  count() %>% 
  mutate(share = n/workrs_num*100)

wl_balance$work_life_balance <- factor(wl_balance$work_life_balance, levels = c("Bad", "Good", "Better", "Best"))
wl_balance <- wl_balance[order(wl_balance$work_life_balance), ]
wl_balance

```


1. Is there any relationship between monthly income and education? Monthly income and gender?

```{r}
hr_cleaned$education <- factor(hr_cleaned$education, levels = c("Below College", "College", "Bachelor", "Master", "Doctor"))
hr_cleaned_srt <- hr_cleaned[order(hr_cleaned$education), ]
hr_cleaned_srt %>% 
  group_by(education) %>% 
  summarise(min_pay = min(monthly_income), median_pay = median(monthly_income), max_pay = max(monthly_income),
            mean_pay = mean(monthly_income), sd_pay = STDEV(monthly_income))


gndr_pay <- hr_cleaned %>% 
  group_by(gender) %>% 
  summarise(med_pay = median(monthly_income))
ggplot(gndr_pay, aes(x = gender, y = med_pay)) +
  geom_col()

hr_cleaned %>% 
  group_by(gender) %>% 
  summarise(min_pay = min(monthly_income), median_pay = median(monthly_income), max_pay = max(monthly_income),
            mean_pay = mean(monthly_income), sd_pay = STDEV(monthly_income))

```


1. Plot a boxplot of income vs job role. Make sure the highest-paid job roles appear first

```{r}
# we compare payment for job roles on median basis

job_pays <- hr_cleaned %>% 
  group_by(job_role) %>% 
  summarise(med_pay = median(monthly_income)) %>% 
  arrange(desc(med_pay))
job_pays

ggplot(hr_cleaned, aes(x = monthly_income, y = reorder(job_role, monthly_income, FUN = median))) +
  geom_boxplot() +
  labs(x = "Monthly Income",
       y = "Job Role") +
  theme(axis.text.x = element_text(size = 8, angle = 45, vjust = 0.9, hjust = 0.8))

```


1. Calculate and plot a bar chart of the mean (or median?) income by education level.

```{r}
# we consider median to be a more robust measure for analysis, so median income is used

med_ed <- hr_cleaned %>% 
  group_by(education) %>% 
  summarise(med_inc = median(monthly_income))

ggplot(med_ed, aes(x = education, y = med_inc)) +
  geom_col()

# # mean analysis is provided below as an alternative option
# mean_ed <- hr_cleaned %>% 
#   group_by(education) %>% 
#   summarise(mean_inc = mean(monthly_income))
# 
# ggplot(mean_ed, aes(x = education, y = mean_inc)) +
#   geom_col()


```


1. Plot the distribution of income by education level. Use a facet_wrap and a theme from `ggthemes`

```{r}
ggplot(hr_cleaned, aes(monthly_income)) +
  geom_density() + 
  facet_wrap(~education) +
  theme_minimal()

```


1. Plot income vs age, faceted by `job_role`

```{r}
ggplot(hr_cleaned, aes(x = age, y = monthly_income)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~job_role)
```


The first observation we can make from the data set is that the attrition rate i.e. the share of employees who left the company is 16.1%. Next, if we look at age, years at the company, monthly income and years since last promotion figures, we can see that of these, age seems to have the most normal distribution. The modal age of the employees is around 34 years, with a slight right skew, the median and mean ages seem to be slightly higher. The distribution of years at the company has a significant right skew, with a mode around 5 years and slight peaks around ‘milestone’ numbers like 10 or 20. This is intuitive as 5, 10 and 20 years may seem to employees as the optimal times to consider a career switch or retiring. The distribution of the monthly income also has a right skew with a modal income of around 3000. This can be linked to the number of employees at each seniority level. As there would be more employees of a more junior level, the modal income should be on the lower side of the income range. The distribution is similar for the years since last promotion, we can see that the majority of employees have been promoted within the last 3 years and very few work for over 8 years without being promoted. This could also be linked to the fact that, without a promotion for many years, employees would leave the company.  
 
Now, if we look at job satisfaction, we would see a left-skewed distribution with the modal job satisfaction as ‘Very High’ with about 31% reporting this level, and a further 30% reporting ‘High’ satisfaction, implying that the majority of employees are happy with their jobs. The work-life balance seems to be slightly more normally distributed with the majority (~60%) reporting a ‘Better’ work-life balance. The distribution has a slight left skew as well, with only 10% reporting a ‘Best’ work-life balance to the right while 23% reporting ‘Good’ and 5% reporting ‘Bad’ to the left. But we can also see that >90% are reporting a ‘Good’, ‘Better’, ‘Best’ work-life balance, implying an overall satisfaction with the work-life balance. 
 
Returning back to monthly income, we can see a clear positive correlation between education level and mean income, with the mean income increasing consistently from 5640 at ‘Below College’ level to 8277 at ‘Doctor’ level. In addition, there seems to be no impact of additional education on the maximum pay, with maximum income between 19500 to 20000 regardless of education level. For the minimum income level, we can only see that ‘Master’ and ‘Doctor’ tend to affect the minimum income level, with the minimum income around 1000 for any education level below ‘Master’. For every level of education, the distribution has a right skew. However, the higher the level of education, the lower the peak of the density distribution, meaning that there is more variation in monthly income, the higher the education level. Moreover, median pay is slightly higher for Females than Males.  
 
If we consider the income by role, we can see that the Manager and Research Director roles are the highest paid on average with Managers having a more consistent pay with a narrower interquartile range. We can also see that for each role the pay consistently increases with age, except for the roles of Laboratory Technician and Sales Representative where the pay seems to be relatively flat as age increases. In addition, the roles of Healthcare Representative, Research Director, Human Resources and Manager tend to have a steeper increase in pay as age increases, however, the variation in monthly pay at every age is higher too. This could be linked to education level and years in the company of employees at every age, as Technician and Sales Rep roles are more likely to be entry-level and hence have less variation.




# Challenge 1: Replicating a chart

The purpose of this exercise is to make a publication-ready plot using your `dplyr` and `ggplot2` skills. Open the journal article "Riddell_Annals_Hom-Sui-Disparities.pdf". Read the abstract and have a look at Figure 3. The data you need is "CDC_Males.csv".

```{r challenge1, echo=FALSE, out.width="90%"}
knitr::include_graphics(here::here("images", "figure3.jpeg"), error = FALSE)
```


Don't worry about replicating it exactly, try and see how far you can get. You're encouraged to work together if you want to and exchange tips/tricks you figured out. 

You may find these helpful:

- https://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.html
- http://colorbrewer2.org



```{r, echo=FALSE}
# Replicate Figure 3
figure_3 <- read_csv(here::here("data", "CDC_Males.csv"))

skim(figure_3)

adj_na <- figure_3 %>% 
  filter(!is.na(gun.house.prev.category)) %>% 
  filter(type == "Firearm")

adj_fig <-select(adj_na, c("ST","adjusted.suicide.White","adjusted.homicide.White", "average.pop.white",
                    "gun.house.prev.category"))

ggplot(adj_fig, aes(x = adjusted.suicide.White, y=adjusted.homicide.White)) + 
  geom_point(aes(size = average.pop.white, fill = gun.house.prev.category), alpha = 0.95, col = "black", pch = 21) +
  scale_fill_manual(values = c("#fef0d9", "#fdcc8a", "#fc8d59", "#d7301f")) + 
  labs(x = "White Suicide Rate (per 100,000 per Year)",
       y = "White Homicide Rate (per 100,000 per Year)") +
  theme_bw() +
  ylim(0.5, 4.9) +
  scale_size_area(breaks = c(500000, 1500000, 3000000, 7000000), 
                  labels = c("500k", "1.5m", "3m", "7m"),
                  max_size = 15) +
  geom_text_repel(label = adj_na$ST, size = 4) +
  guides(fill = guide_legend(title = "Gun ownership",
                             override.aes = list(size = 7), order = 1),
         size = guide_legend(title = "White population"), order = 2) +
  geom_text(aes(x = 23, y = 0.7, 
                label = paste0("Spearman's rho: 0.74")),
            check_overlap = T) +
  coord_fixed(ratio = 6)



```



# Challenge 2: 2016 California Contributors plots

As discussed in class, I would like you to reproduce the plot that shows the top ten cities in highest amounts raised in political contributions in California during the 2016 US Presidential election.



```{r challenge2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "challenge2.png"), error = FALSE)
```


To get this plot, you must join two dataframes; the one you have with all contributions, and data that can translate zipcodes to cities.
You can find a file with all US zipcodes, e.g., here http://www.uszipcodelist.com/download.html. 

The easiest way would be to create two plots and then place one next to each other. For this, you will need the `patchwork` package.
https://cran.r-project.org/web/packages/patchwork/index.html

While this is ok, what if one asked you to create the same plot for the top 10 candidates and not just the top two? The most challenging part is how to reorder within categories, and for this you will find Julia Silge's post on [REORDERING AND FACETTING FOR GGPLOT2](https://juliasilge.com/blog/reorder-within/) useful.


```{r, load_CA_data, warnings= FALSE, message=FALSE}
library("patchwork")

# Make sure you use vroom() as it is significantly faster than read.csv()

CA_contributors_2016 <- vroom::vroom(here::here("data","CA_contributors_2016.csv"))
zip_code <- vroom::vroom(here::here("data", "zip_code_database.csv"))
zip_code <- select(zip_code, c("zip", "primary_city"))

figure <- merge(CA_contributors_2016, zip_code, by.x = "zip")

# Hillary Clinton Calculation
col_gr <- figure %>% 
  filter(cand_nm == "Clinton, Hillary Rodham") %>% 
  group_by(primary_city) %>% 
  summarise(total_contb = sum(contb_receipt_amt)) %>% 
  arrange(desc(total_contb)) %>% 
  head(10)

hl_cl <- ggplot(col_gr, aes(x = total_contb, y = fct_reorder(primary_city, total_contb))) +
  geom_col(fill = "blue") +
  theme_bw() +
  labs(title = "Where did candidates raise most money?",
       subtitle = "Clinton, Hillary Rodham",
       x = "",
       y = "")

# Donald Trump Calculation
col_gr_T <- figure %>% 
  filter(cand_nm == "Trump, Donald J.") %>% 
  group_by(primary_city) %>% 
  summarise(total_contb = sum(contb_receipt_amt)) %>% 
  arrange(desc(total_contb)) %>% 
  head(10)

dn_tr <- ggplot(col_gr_T, aes(x = total_contb, y = fct_reorder(primary_city, total_contb))) +
  geom_col(fill = "red") +
  theme_bw() +
  labs(title = "",
       subtitle = "Trump, Donald Jr.",
       x = "",
       y = "")

hl_cl + dn_tr





# Calculation for everyone at once
library(tidytext)

sum_data <- figure %>% 
  group_by(cand_nm, primary_city) %>% 
  summarise(ttl_contb = sum(contb_receipt_amt))

sum_data %>%
    group_by(cand_nm) %>%
    top_n(10) %>%
    ungroup %>%
    mutate(cand_nm = as.factor(cand_nm),
           primary_city = reorder_within(primary_city, ttl_contb, cand_nm)) %>%
    ggplot(aes(primary_city, ttl_contb, fill = cand_nm)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~cand_nm, scales = "free") +
    coord_flip() +
    scale_x_reordered() +
    scale_y_continuous() +
    labs(y = "Amount Raised",
         x = NULL,
         title = "Where did candidates raise most money?")

```




# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.

# Details

- Who did you collaborate with: Benedikt Jaletzke, Stanislav Makarov, Mark Negodyuk, Olivia Zhang, Kateryna Tarasova
- Approximately how much time did you spend on this problem set: 6-7 hours
- What, if anything, gave you the most trouble: ANSWER HERE


**Please seek out help when you need it,** and remember the [15-minute rule](https://mam2021.netlify.app/syllabus/#the-15-minute-rule){target=_blank}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!  

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else? 


# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. 

Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). 

Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output.









